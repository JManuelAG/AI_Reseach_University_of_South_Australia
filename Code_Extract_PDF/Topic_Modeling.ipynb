{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jmanu\\anaconda3\\lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jmanu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jmanu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\jmanu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "import gensim\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')  \n",
    "nltk.download('omw-1.4')  \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "import string\n",
    "import json \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"../Datasets/PubMed/PubMed_20k_RCT/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sentences as Doc\n",
    "# abstracts = df.abstract_text.values.tolist()\n",
    "\n",
    "# # Abstract as Doc\n",
    "abstracts = df.groupby('abstract_id')['abstract_text'].apply(' '.join)\n",
    "articles = abstracts.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./articles.json\", 'r') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abstract As documents\n",
    "articles = [data[i][\"Abstract\"] for i in range(len(data))]\n",
    "\n",
    "\n",
    "# All sentences of Abstract as Document\n",
    "#articles = [sentence for i in range(len(data)) for sentence in data[i][\"Abstract\"].split(\"\\n\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords, punctuation, and normalize the corpus\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = \"\".join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "clean_corpus = [clean(doc).split() for doc in articles]\n",
    "\n",
    "# Step 2: Create Dictionary and Corpus\n",
    "# Creating document-term matrix \n",
    "dictionary = corpora.Dictionary(clean_corpus)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in clean_corpus]\n",
    "\n",
    "# Step 3: Build LDA model\n",
    "from gensim.models import LsiModel\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=doc_term_matrix, id2word=dictionary, random_state=42,num_topics=5, update_every=1, chunksize=100, passes=10)\n",
    "#lsa = LsiModel(doc_term_matrix, num_topics=3, id2word = dictionary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.029*\"digital\" + 0.023*\"twin\" + 0.021*\"process\" + 0.013*\"medical\" + 0.011*\"model\" + 0.011*\"used\" + 0.010*\"physical\" + 0.010*\"data\"\n",
      "Topic: 1 \n",
      "Words: 0.017*\"student\" + 0.015*\"ecases\" + 0.013*\"ecg\" + 0.012*\"clinical\" + 0.011*\"classification\" + 0.011*\"soo\" + 0.011*\"otva\" + 0.010*\"machine\"\n",
      "Topic: 2 \n",
      "Words: 0.036*\"patient\" + 0.026*\"pembrolizumab\" + 0.020*\"progression\" + 0.020*\"pd\" + 0.016*\"salvage\" + 0.015*\"study\" + 0.014*\"trial\" + 0.014*\"beyond\"\n",
      "Topic: 3 \n",
      "Words: 0.013*\"model\" + 0.012*\"data\" + 0.012*\"health\" + 0.009*\"digital\" + 0.008*\"system\" + 0.008*\"technology\" + 0.007*\"twin\" + 0.007*\"care\"\n",
      "Topic: 4 \n",
      "Words: 0.038*\"stage\" + 0.038*\"reversal\" + 0.022*\"patient\" + 0.020*\"diabetes\" + 0.020*\"study\" + 0.020*\"therapy\" + 0.018*\"2\" + 0.015*\"nutrition\"\n"
     ]
    }
   ],
   "source": [
    "# View the topics to understand what each topic represents\n",
    "for idx, topic in lda_model.print_topics(-1, num_words=8):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_labels = {\n",
    "    0: 'Digital Twin',\n",
    "    1: 'Clinical Trail',\n",
    "    2: 'Patient Study',\n",
    "    3: 'AI in Health',\n",
    "    4: 'Medical Recovery'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstract All people are unique and so are their diseases.\n",
      "Our genomes, disease histories, behavior, and lifestyles are all different; therefore it is not too surprising that people often respond differently when administered the same drugs.\n",
      "Cancer, in particular, is a complex and heterogeneous disease, originating in patients with different genomes, in cells with the different epigenomes, formed and evolving on the basis of random processes, with the response to therapy not only depending on the individual cancer cell but also on many features of the patient.\n",
      "Selection of an optimal therapy will therefore require a deep molecular analysis comprising both the patient and their tumor (e.g., comprehensive molecular tumor analysis [CMTA]), and much better personalized prediction of response to possible therapies.            As the knowledge base on cancer, cellular transduction and molecular interactions widens, so does our ability to generate computational models with the capacity to accurately represent the complex networks and cross-talk determining cancer progression and drug response [            hTe main advantages of mechanistic models are the integration of data from Currently, we are at an inflection point in which advances in technology, decreases in the costs of sequencing and other molecular analyses, and increases in computing advances are converging, forming the foundation to build a data-driven approach to personalized oncology.\n",
      "In this article we discuss the deep molecular characterization of individual tumors and patients as the basis of not only current precision oncology but also of computational models (‘digital twins’), the foundation for a truly personalized therapy selection of the future.\n"
     ]
    }
   ],
   "source": [
    "print(articles[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0 is labeled as: Digital Twin\n",
      "Document 1 is labeled as: AI in Health\n",
      "Document 2 is labeled as: Digital Twin\n",
      "Document 3 is labeled as: Patient Study\n",
      "Document 4 is labeled as: AI in Health\n",
      "Document 5 is labeled as: Patient Study\n",
      "Document 6 is labeled as: AI in Health\n",
      "Document 7 is labeled as: AI in Health\n",
      "Document 8 is labeled as: AI in Health\n",
      "Document 9 is labeled as: AI in Health\n",
      "Document 10 is labeled as: AI in Health\n",
      "Document 11 is labeled as: Digital Twin\n",
      "Document 12 is labeled as: AI in Health\n",
      "Document 13 is labeled as: AI in Health\n",
      "Document 14 is labeled as: Patient Study\n",
      "Document 15 is labeled as: Patient Study\n",
      "Document 16 is labeled as: Clinical Trail\n",
      "Document 17 is labeled as: AI in Health\n",
      "Document 18 is labeled as: Patient Study\n",
      "Document 19 is labeled as: AI in Health\n",
      "Document 20 is labeled as: Digital Twin\n",
      "Document 21 is labeled as: Digital Twin\n",
      "Document 22 is labeled as: AI in Health\n",
      "Document 23 is labeled as: Clinical Trail\n",
      "Document 24 is labeled as: Medical Recovery\n",
      "Document 25 is labeled as: Patient Study\n",
      "Document 26 is labeled as: AI in Health\n",
      "Document 27 is labeled as: AI in Health\n",
      "Document 28 is labeled as: AI in Health\n",
      "Document 29 is labeled as: Medical Recovery\n",
      "Document 30 is labeled as: AI in Health\n",
      "Document 31 is labeled as: Digital Twin\n",
      "Document 32 is labeled as: Digital Twin\n",
      "Document 33 is labeled as: Patient Study\n",
      "Document 34 is labeled as: Patient Study\n",
      "Document 35 is labeled as: Digital Twin\n",
      "Document 36 is labeled as: AI in Health\n",
      "Document 37 is labeled as: Digital Twin\n",
      "Document 38 is labeled as: AI in Health\n",
      "Document 39 is labeled as: Medical Recovery\n",
      "Document 40 is labeled as: AI in Health\n",
      "Document 41 is labeled as: Patient Study\n",
      "Document 42 is labeled as: AI in Health\n",
      "Document 43 is labeled as: Digital Twin\n",
      "Document 44 is labeled as: Digital Twin\n",
      "Document 45 is labeled as: AI in Health\n",
      "Document 46 is labeled as: Patient Study\n",
      "Document 47 is labeled as: Digital Twin\n",
      "Document 48 is labeled as: AI in Health\n",
      "Document 49 is labeled as: AI in Health\n",
      "Document 50 is labeled as: Patient Study\n",
      "Document 51 is labeled as: AI in Health\n",
      "Document 52 is labeled as: AI in Health\n",
      "Document 53 is labeled as: Digital Twin\n",
      "Document 54 is labeled as: AI in Health\n",
      "Document 55 is labeled as: Digital Twin\n",
      "Document 56 is labeled as: AI in Health\n",
      "Document 57 is labeled as: Digital Twin\n",
      "Document 58 is labeled as: AI in Health\n",
      "Document 59 is labeled as: Digital Twin\n",
      "Document 60 is labeled as: Clinical Trail\n",
      "Document 61 is labeled as: AI in Health\n",
      "Document 62 is labeled as: AI in Health\n",
      "Document 63 is labeled as: Digital Twin\n",
      "Document 64 is labeled as: AI in Health\n",
      "Document 65 is labeled as: Patient Study\n",
      "Document 66 is labeled as: Digital Twin\n",
      "Document 67 is labeled as: Patient Study\n",
      "Document 68 is labeled as: AI in Health\n",
      "Document 69 is labeled as: AI in Health\n",
      "Document 70 is labeled as: Clinical Trail\n",
      "Document 71 is labeled as: Digital Twin\n",
      "Document 72 is labeled as: Digital Twin\n",
      "Document 73 is labeled as: AI in Health\n",
      "Document 74 is labeled as: Patient Study\n",
      "Document 75 is labeled as: AI in Health\n",
      "Document 76 is labeled as: Digital Twin\n",
      "Document 77 is labeled as: Digital Twin\n",
      "Document 78 is labeled as: AI in Health\n",
      "Document 79 is labeled as: AI in Health\n",
      "Document 80 is labeled as: Digital Twin\n",
      "Document 81 is labeled as: Digital Twin\n",
      "Document 82 is labeled as: Digital Twin\n",
      "Document 83 is labeled as: Digital Twin\n",
      "Document 84 is labeled as: Medical Recovery\n",
      "Document 85 is labeled as: Medical Recovery\n",
      "Document 86 is labeled as: AI in Health\n",
      "Document 87 is labeled as: AI in Health\n",
      "Document 88 is labeled as: AI in Health\n",
      "Document 89 is labeled as: AI in Health\n",
      "Document 90 is labeled as: Clinical Trail\n",
      "Document 91 is labeled as: Digital Twin\n",
      "Document 92 is labeled as: AI in Health\n",
      "Document 93 is labeled as: AI in Health\n",
      "Document 94 is labeled as: AI in Health\n",
      "Document 95 is labeled as: AI in Health\n",
      "Document 96 is labeled as: Digital Twin\n",
      "Document 97 is labeled as: Patient Study\n",
      "Document 98 is labeled as: Digital Twin\n",
      "Document 99 is labeled as: AI in Health\n",
      "Document 100 is labeled as: Clinical Trail\n",
      "Document 101 is labeled as: Medical Recovery\n",
      "Document 102 is labeled as: Digital Twin\n",
      "Document 103 is labeled as: Clinical Trail\n",
      "Document 104 is labeled as: Patient Study\n",
      "Document 105 is labeled as: Digital Twin\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "# Example of mapping a document to a topic label\n",
    "for i, row_list in enumerate(lda_model[doc_term_matrix]):\n",
    "    row = row_list[0] if lda_model.per_word_topics else row_list            \n",
    "    row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "    dominant_topic = int(row[0][0])\n",
    "    label = topic_labels[dominant_topic]\n",
    "    results.append(label)\n",
    "    print(f\"Document {i} is labeled as: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(results)\n",
    "save_test(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_test(answers):\n",
    "    file_path = \"../Test_Results/Test Results.xlsx\"\n",
    "    sheet_name = \"class_lab\"\n",
    "    answers.insert(0, \"Topic Modeling\")\n",
    "    new_data = pd.DataFrame([answers])\n",
    "\n",
    "    # Try to read the existing data\n",
    "    try:\n",
    "        existing_data = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "    except FileNotFoundError:\n",
    "        existing_data = pd.DataFrame()\n",
    "\n",
    "    # Append new data\n",
    "    updated_data = pd.concat([existing_data, new_data], ignore_index=True)\n",
    "    updated_data = updated_data.drop_duplicates()\n",
    "\n",
    "    # Write back to Excel\n",
    "    with pd.ExcelWriter(file_path, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n",
    "        updated_data.to_excel(writer, sheet_name=sheet_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming lda_model and corpus are already created as shown in the previous example\n",
    "\n",
    "# Function to get the dominant topic for each document\n",
    "def format_topics_sentences(ldamodel, corpus):\n",
    "    # Init output\n",
    "    topics_data = []\n",
    "    for i, row_list in enumerate(ldamodel[corpus]):\n",
    "        row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the dominant topic\n",
    "        dominant_topic = int(row[0][0])\n",
    "        topics_data.append(dominant_topic)\n",
    "        \n",
    "    return topics_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Get dominant topic for each document\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m dominant_topics \u001b[38;5;241m=\u001b[39m \u001b[43mformat_topics_sentences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlda_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclean_corpus\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36mformat_topics_sentences\u001b[1;34m(ldamodel, corpus)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat_topics_sentences\u001b[39m(ldamodel, corpus):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Init output\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     topics_data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, row_list \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mldamodel\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m]\u001b[49m):\n\u001b[0;32m      8\u001b[0m         row \u001b[38;5;241m=\u001b[39m row_list[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m ldamodel\u001b[38;5;241m.\u001b[39mper_word_topics \u001b[38;5;28;01melse\u001b[39;00m row_list            \n\u001b[0;32m      9\u001b[0m         \u001b[38;5;66;03m# Get the Dominant topic, Perc Contribution and Keywords for each document\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jmanu\\anaconda3\\lib\\site-packages\\gensim\\models\\ldamodel.py:1548\u001b[0m, in \u001b[0;36mLdaModel.__getitem__\u001b[1;34m(self, bow, eps)\u001b[0m\n\u001b[0;32m   1527\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, bow, eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1528\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the topic distribution for the given document.\u001b[39;00m\n\u001b[0;32m   1529\u001b[0m \n\u001b[0;32m   1530\u001b[0m \u001b[38;5;124;03m    Wraps :meth:`~gensim.models.ldamodel.LdaModel.get_document_topics` to support an operator style call.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1546\u001b[0m \n\u001b[0;32m   1547\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1548\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_document_topics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimum_phi_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mper_word_topics\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jmanu\\anaconda3\\lib\\site-packages\\gensim\\models\\ldamodel.py:1353\u001b[0m, in \u001b[0;36mLdaModel.get_document_topics\u001b[1;34m(self, bow, minimum_probability, minimum_phi_value, per_word_topics)\u001b[0m\n\u001b[0;32m   1346\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m   1347\u001b[0m         per_word_topics\u001b[38;5;241m=\u001b[39mper_word_topics,\n\u001b[0;32m   1348\u001b[0m         minimum_probability\u001b[38;5;241m=\u001b[39mminimum_probability,\n\u001b[0;32m   1349\u001b[0m         minimum_phi_value\u001b[38;5;241m=\u001b[39mminimum_phi_value\n\u001b[0;32m   1350\u001b[0m     )\n\u001b[0;32m   1351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(corpus, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 1353\u001b[0m gamma, phis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbow\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollect_sstats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mper_word_topics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1354\u001b[0m topic_dist \u001b[38;5;241m=\u001b[39m gamma[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;28msum\u001b[39m(gamma[\u001b[38;5;241m0\u001b[39m])  \u001b[38;5;66;03m# normalize distribution\u001b[39;00m\n\u001b[0;32m   1356\u001b[0m document_topics \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   1357\u001b[0m     (topicid, topicvalue) \u001b[38;5;28;01mfor\u001b[39;00m topicid, topicvalue \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(topic_dist)\n\u001b[0;32m   1358\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m topicvalue \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m minimum_probability\n\u001b[0;32m   1359\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\jmanu\\anaconda3\\lib\\site-packages\\gensim\\models\\ldamodel.py:698\u001b[0m, in \u001b[0;36mLdaModel.inference\u001b[1;34m(self, chunk, collect_sstats)\u001b[0m\n\u001b[0;32m    695\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d, doc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(chunk):\n\u001b[0;32m    696\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(doc) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(doc[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m], integer_types):\n\u001b[0;32m    697\u001b[0m         \u001b[38;5;66;03m# make sure the term IDs are ints, otherwise np will get upset\u001b[39;00m\n\u001b[1;32m--> 698\u001b[0m         ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mint\u001b[39m(idx) \u001b[38;5;28;01mfor\u001b[39;00m idx, _ \u001b[38;5;129;01min\u001b[39;00m doc]\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    700\u001b[0m         ids \u001b[38;5;241m=\u001b[39m [idx \u001b[38;5;28;01mfor\u001b[39;00m idx, _ \u001b[38;5;129;01min\u001b[39;00m doc]\n",
      "File \u001b[1;32mc:\\Users\\jmanu\\anaconda3\\lib\\site-packages\\gensim\\models\\ldamodel.py:698\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    695\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d, doc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(chunk):\n\u001b[0;32m    696\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(doc) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(doc[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m], integer_types):\n\u001b[0;32m    697\u001b[0m         \u001b[38;5;66;03m# make sure the term IDs are ints, otherwise np will get upset\u001b[39;00m\n\u001b[1;32m--> 698\u001b[0m         ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mint\u001b[39m(idx) \u001b[38;5;28;01mfor\u001b[39;00m idx, _ \u001b[38;5;129;01min\u001b[39;00m doc]\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    700\u001b[0m         ids \u001b[38;5;241m=\u001b[39m [idx \u001b[38;5;28;01mfor\u001b[39;00m idx, _ \u001b[38;5;129;01min\u001b[39;00m doc]\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# Get dominant topic for each document\n",
    "dominant_topics = format_topics_sentences(lda_model, clean_corpus)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 4]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dominant_topics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
