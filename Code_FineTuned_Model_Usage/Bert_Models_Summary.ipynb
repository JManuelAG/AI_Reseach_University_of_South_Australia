{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel, AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code uses two BERT models and the use of cosine similarity to create a summary from original text.  \n",
    "The summary will be done with some basic functions and choosing most relevant sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular Large model \n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
    "model = BertModel.from_pretrained('bert-large-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clinical Bert\n",
    "tokenizer = AutoTokenizer.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')\n",
    "model = AutoModel.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data \n",
    "file_path = \"\"\n",
    "with open(file_path, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    # process the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a sentence\n",
    "text = lines[0]\n",
    "\n",
    "# Split text into sentences\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "# Encode sentences\n",
    "encoded_sentences = [tokenizer.encode(sentence, return_tensors='pt', add_special_tokens=True) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to embeddings\n",
    "sentence_embeddings = []\n",
    "for encoded_sentence in encoded_sentences:\n",
    "    with torch.no_grad():\n",
    "        output = model(encoded_sentence)\n",
    "    # Using [CLS] token embedding for representing the sentence\n",
    "    sentence_embeddings.append(output[0][:, 0, :])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Calculate mean embedding for the document\n",
    "doc_embedding = torch.mean(torch.cat(sentence_embeddings, dim=0), dim=0)\n",
    "\n",
    "# Calculate cosine similarity of each sentence with the document\n",
    "similarities = [cosine_similarity(embedding, doc_embedding.unsqueeze(0))[0][0] for embedding in sentence_embeddings]\n",
    "\n",
    "# Select top N sentences\n",
    "N = round(len(sentences)/2)  # number of sentences to include in the summary\n",
    "top_sentences = np.argsort(similarities)[-N:]\n",
    "summary = \" \".join([sentences[i] for i in sorted(top_sentences)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Retained endobronchial foreign body removal facilitated by steroid therapy of an obstructing, inflammatory polyp. Oral and topical steroids were used to induce regression in an inflammatory, obstructing endobronchial polyp caused by a retained foreign body. The FB (a peanut half), which had been present for over six months, was then able to be easily and bloodlessly retrieved with fiberoptic bronchoscopy. \\n'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the original Text\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Preoperatively, all of the affected eyes had worse visual acuity (P less than .02) and more astigmatism (P less than .01) than the contralateral eyes. Of the eight patients for whom both preoperative and postoperative visual acuity measurements had been obtained, in six it had changed minimally (less than or equal to 1 line), and in two it had improved (less than or equal to 2 lines). Surgical complications included persistent epithelial defects (40%) and peripheral corneal vascularization and opacity (70%). These complications do not outweigh the cosmetic and visual benefits of dermoid excision in selected patients.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the summary of the text\n",
    "summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
